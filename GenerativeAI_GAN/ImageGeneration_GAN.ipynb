{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df2821d0",
   "metadata": {},
   "source": [
    "## Michael Owen GAN Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554dbac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, LeakyReLU, Reshape, AveragePooling2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Flatten, Dense, Input, Activation,MaxPool2D, BatchNormalization, Dropout\n",
    "import tensorflow_docs.vis.embed as embed\n",
    "import glob, os, imageio\n",
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2cf7f79",
   "metadata": {},
   "source": [
    "## Tensorflow Image generation via two competing networks (generator and discriminator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14bc6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=\"he_uniform\", input_shape=(28,28,1)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=\"he_uniform\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(128, (4,4), strides=(1,1), padding='same', kernel_initializer=\"he_uniform\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=\"glorot_uniform\"))\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "def get_generator(lat_space):\n",
    "    model = Sequential()\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, kernel_initializer=\"he_uniform\",activation='relu', input_dim=lat_space))\n",
    "    #model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(Conv2DTranspose(256, (4,4), strides=(2,2), padding='same', activation='relu', kernel_initializer=\"he_uniform\"))\n",
    "    #model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', activation='relu', kernel_initializer=\"he_uniform\"))\n",
    "    #model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(1,1), padding='same', activation='relu', kernel_initializer=\"he_uniform\"))\n",
    "    #model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=\"glorot_uniform\"))\n",
    "    return model\n",
    "\n",
    "def get_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def get_data():\n",
    "    (trainX, trainy), (_, _) = mnist.load_data()\n",
    "    X_t = np.expand_dims(trainX, axis=3)\n",
    "    X_t = X_t.astype('float32')\n",
    "    # [0,255] -> [-1,1]\n",
    "    X_t = (X_t - 127.5) / 127.5\n",
    "    return X_t\n",
    "\n",
    "def get_real_imgs(dataset, n_samples):\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    X_r = dataset[ix]\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X_r, y\n",
    "\n",
    " \n",
    "def get_fake_imgs(generator, lat_space, n_samples):\n",
    "    x_input = np.random.normal(0,1,size=[n_samples,lat_space])\n",
    "    X_f = generator.predict(x_input)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X_f, y\n",
    "\n",
    "def save_gen_imgs(step, g_model, lat_space, n_samples=200):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    X, _ = get_fake_imgs(g_model, lat_space, n_samples)\n",
    "    X = (X + 1) / 2.0\n",
    "    X = X.reshape(n_samples,28,28)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.subplot(10, 20, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X[i], cmap='gray')\n",
    "    plt.savefig('results_new9/generated_plot_%03d.png' % (step+1))\n",
    "    plt.close()\n",
    "    \n",
    "def loss_acc_plot(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(d1_hist, label='d-real')\n",
    "    plt.plot(d2_hist, label='d-fake')\n",
    "    plt.plot(g_hist, label='gen')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(a1_hist, label='acc-real')\n",
    "    plt.plot(a2_hist, label='acc-fake')\n",
    "    plt.legend()\n",
    "    plt.savefig('results_new9/plot_line_plot_loss.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945774c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan():\n",
    "    n_epochs=200\n",
    "    lat_space = 150\n",
    "    discriminator = get_discriminator()\n",
    "    generator = get_generator(lat_space)\n",
    "    gan_combined = get_gan(generator, discriminator)\n",
    "    data = get_data()\n",
    "    n_batch = 256\n",
    "    bat_per_epo = int(data.shape[0] / n_batch)\n",
    "    iterations = bat_per_epo * n_epochs\n",
    "    half_batch = int(n_batch / 2)\n",
    "    d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n",
    "    for i in range(iterations):\n",
    "        ind = np.random.randint(0,data.shape[0],half_batch)\n",
    "        X_real, y_real = get_real_imgs(data[ind], half_batch)\n",
    "        \n",
    "        d_loss1, d_acc1 = discriminator.train_on_batch(X_real, y_real)\n",
    "        \n",
    "        X_fake, y_fake = get_fake_imgs(generator, lat_space, half_batch)\n",
    "        \n",
    "        d_loss2, d_acc2 = discriminator.train_on_batch(X_fake, y_fake)\n",
    "        \n",
    "        X_gan_combined = np.random.normal(0,1,size=[n_batch,lat_space])\n",
    "        \n",
    "        y_gan_combined = np.ones((n_batch, 1))\n",
    "        \n",
    "        g_loss = gan_combined.train_on_batch(X_gan_combined, y_gan_combined)\n",
    "        # print batch performance\n",
    "        if (i+1) % 300 == 0 or (i+1) == 100:\n",
    "            print('Step%d, d_r_loss=%.3f, d_f_loss=%.3f g_loss=%.3f, d_r_acc=%d, d_f_acc=%d' %(i+1, d_loss1, d_loss2, g_loss, int(100*d_acc1), int(100*d_acc2)))\n",
    "        # record history\n",
    "        d1_hist.append(d_loss1)\n",
    "        d2_hist.append(d_loss2)\n",
    "        g_hist.append(g_loss)\n",
    "        a1_hist.append(d_acc1)\n",
    "        a2_hist.append(d_acc2)\n",
    "        if (i+1) % 400 == 0 or (i+1) == 1 or (i+1) == 10 or (i+1) == 100 or (i+1) == 150 or (i+1) >=46790:\n",
    "            save_gen_imgs(i, generator, lat_space)\n",
    "    loss_acc_plot(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0f9dd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step100, d_r_loss=0.233, d_f_loss=0.810 g_loss=0.683, d_r_acc=88, d_f_acc=4\n",
      "Step300, d_r_loss=0.466, d_f_loss=0.348 g_loss=1.982, d_r_acc=77, d_f_acc=88\n",
      "Step600, d_r_loss=0.377, d_f_loss=0.349 g_loss=2.296, d_r_acc=81, d_f_acc=89\n",
      "Step900, d_r_loss=0.376, d_f_loss=0.399 g_loss=1.830, d_r_acc=85, d_f_acc=92\n",
      "Step1200, d_r_loss=0.541, d_f_loss=0.416 g_loss=1.535, d_r_acc=70, d_f_acc=87\n",
      "Step1500, d_r_loss=0.680, d_f_loss=0.493 g_loss=1.370, d_r_acc=59, d_f_acc=80\n",
      "Step1800, d_r_loss=0.532, d_f_loss=0.523 g_loss=1.293, d_r_acc=64, d_f_acc=80\n",
      "Step2100, d_r_loss=0.541, d_f_loss=0.512 g_loss=1.321, d_r_acc=68, d_f_acc=77\n",
      "Step2400, d_r_loss=0.562, d_f_loss=0.524 g_loss=1.194, d_r_acc=75, d_f_acc=78\n",
      "Step2700, d_r_loss=0.665, d_f_loss=0.602 g_loss=1.130, d_r_acc=60, d_f_acc=71\n",
      "Step3000, d_r_loss=0.734, d_f_loss=0.599 g_loss=1.321, d_r_acc=56, d_f_acc=67\n",
      "Step3300, d_r_loss=0.619, d_f_loss=0.508 g_loss=1.178, d_r_acc=64, d_f_acc=80\n",
      "Step3600, d_r_loss=0.625, d_f_loss=0.562 g_loss=1.101, d_r_acc=59, d_f_acc=72\n",
      "Step3900, d_r_loss=0.674, d_f_loss=0.612 g_loss=1.008, d_r_acc=60, d_f_acc=66\n",
      "Step4200, d_r_loss=0.677, d_f_loss=0.573 g_loss=1.095, d_r_acc=54, d_f_acc=70\n",
      "Step4500, d_r_loss=0.656, d_f_loss=0.659 g_loss=0.991, d_r_acc=66, d_f_acc=61\n",
      "Step4800, d_r_loss=0.560, d_f_loss=0.606 g_loss=1.048, d_r_acc=71, d_f_acc=68\n",
      "Step5100, d_r_loss=0.644, d_f_loss=0.599 g_loss=1.008, d_r_acc=49, d_f_acc=70\n",
      "Step5400, d_r_loss=0.698, d_f_loss=0.639 g_loss=0.986, d_r_acc=52, d_f_acc=70\n",
      "Step5700, d_r_loss=0.578, d_f_loss=0.682 g_loss=0.999, d_r_acc=64, d_f_acc=59\n",
      "Step6000, d_r_loss=0.641, d_f_loss=0.649 g_loss=0.941, d_r_acc=72, d_f_acc=60\n",
      "Step6300, d_r_loss=0.690, d_f_loss=0.555 g_loss=1.038, d_r_acc=48, d_f_acc=75\n",
      "Step6600, d_r_loss=0.619, d_f_loss=0.646 g_loss=0.914, d_r_acc=61, d_f_acc=64\n",
      "Step6900, d_r_loss=0.631, d_f_loss=0.636 g_loss=0.923, d_r_acc=59, d_f_acc=68\n",
      "Step7200, d_r_loss=0.639, d_f_loss=0.669 g_loss=0.889, d_r_acc=62, d_f_acc=60\n",
      "Step7500, d_r_loss=0.564, d_f_loss=0.677 g_loss=0.879, d_r_acc=77, d_f_acc=56\n",
      "Step7800, d_r_loss=0.590, d_f_loss=0.646 g_loss=0.926, d_r_acc=68, d_f_acc=65\n",
      "Step8100, d_r_loss=0.649, d_f_loss=0.681 g_loss=0.890, d_r_acc=57, d_f_acc=57\n",
      "Step8400, d_r_loss=0.585, d_f_loss=0.649 g_loss=0.886, d_r_acc=67, d_f_acc=58\n",
      "Step8700, d_r_loss=0.725, d_f_loss=0.656 g_loss=0.843, d_r_acc=43, d_f_acc=60\n",
      "Step9000, d_r_loss=0.697, d_f_loss=0.679 g_loss=0.896, d_r_acc=51, d_f_acc=57\n",
      "Step9300, d_r_loss=0.612, d_f_loss=0.701 g_loss=0.879, d_r_acc=71, d_f_acc=52\n",
      "Step9600, d_r_loss=0.674, d_f_loss=0.585 g_loss=0.891, d_r_acc=58, d_f_acc=74\n",
      "Step9900, d_r_loss=0.668, d_f_loss=0.648 g_loss=0.871, d_r_acc=54, d_f_acc=62\n",
      "Step10200, d_r_loss=0.671, d_f_loss=0.651 g_loss=0.830, d_r_acc=50, d_f_acc=64\n",
      "Step10500, d_r_loss=0.686, d_f_loss=0.658 g_loss=0.873, d_r_acc=50, d_f_acc=61\n",
      "Step10800, d_r_loss=0.691, d_f_loss=0.694 g_loss=0.809, d_r_acc=56, d_f_acc=60\n",
      "Step11100, d_r_loss=0.690, d_f_loss=0.689 g_loss=0.814, d_r_acc=50, d_f_acc=60\n",
      "Step11400, d_r_loss=0.652, d_f_loss=0.651 g_loss=0.862, d_r_acc=57, d_f_acc=63\n",
      "Step11700, d_r_loss=0.701, d_f_loss=0.706 g_loss=0.824, d_r_acc=53, d_f_acc=50\n",
      "Step12000, d_r_loss=0.695, d_f_loss=0.700 g_loss=0.838, d_r_acc=51, d_f_acc=54\n",
      "Step12300, d_r_loss=0.702, d_f_loss=0.667 g_loss=0.845, d_r_acc=41, d_f_acc=57\n",
      "Step12600, d_r_loss=0.614, d_f_loss=0.647 g_loss=0.823, d_r_acc=67, d_f_acc=63\n",
      "Step12900, d_r_loss=0.648, d_f_loss=0.661 g_loss=0.807, d_r_acc=60, d_f_acc=60\n",
      "Step13200, d_r_loss=0.662, d_f_loss=0.684 g_loss=0.825, d_r_acc=60, d_f_acc=55\n",
      "Step13500, d_r_loss=0.715, d_f_loss=0.656 g_loss=0.876, d_r_acc=41, d_f_acc=64\n",
      "Step13800, d_r_loss=0.665, d_f_loss=0.655 g_loss=0.837, d_r_acc=62, d_f_acc=60\n",
      "Step14100, d_r_loss=0.674, d_f_loss=0.678 g_loss=0.834, d_r_acc=49, d_f_acc=64\n",
      "Step14400, d_r_loss=0.671, d_f_loss=0.675 g_loss=0.811, d_r_acc=51, d_f_acc=60\n",
      "Step14700, d_r_loss=0.665, d_f_loss=0.668 g_loss=0.832, d_r_acc=60, d_f_acc=54\n",
      "Step15000, d_r_loss=0.718, d_f_loss=0.691 g_loss=0.801, d_r_acc=49, d_f_acc=52\n",
      "Step15300, d_r_loss=0.716, d_f_loss=0.681 g_loss=0.764, d_r_acc=45, d_f_acc=60\n",
      "Step15600, d_r_loss=0.713, d_f_loss=0.666 g_loss=0.802, d_r_acc=49, d_f_acc=60\n",
      "Step15900, d_r_loss=0.646, d_f_loss=0.656 g_loss=0.819, d_r_acc=48, d_f_acc=68\n",
      "Step16200, d_r_loss=0.632, d_f_loss=0.681 g_loss=0.748, d_r_acc=62, d_f_acc=53\n",
      "Step16500, d_r_loss=0.673, d_f_loss=0.659 g_loss=0.838, d_r_acc=57, d_f_acc=61\n",
      "Step16800, d_r_loss=0.696, d_f_loss=0.634 g_loss=0.834, d_r_acc=43, d_f_acc=70\n",
      "Step17100, d_r_loss=0.672, d_f_loss=0.690 g_loss=0.808, d_r_acc=57, d_f_acc=57\n",
      "Step17400, d_r_loss=0.714, d_f_loss=0.696 g_loss=0.809, d_r_acc=49, d_f_acc=57\n",
      "Step17700, d_r_loss=0.656, d_f_loss=0.688 g_loss=0.767, d_r_acc=55, d_f_acc=52\n",
      "Step18000, d_r_loss=0.651, d_f_loss=0.699 g_loss=0.772, d_r_acc=56, d_f_acc=52\n",
      "Step18300, d_r_loss=0.673, d_f_loss=0.674 g_loss=0.794, d_r_acc=51, d_f_acc=60\n",
      "Step18600, d_r_loss=0.728, d_f_loss=0.675 g_loss=0.795, d_r_acc=46, d_f_acc=58\n",
      "Step18900, d_r_loss=0.659, d_f_loss=0.669 g_loss=0.813, d_r_acc=56, d_f_acc=62\n",
      "Step19200, d_r_loss=0.692, d_f_loss=0.686 g_loss=0.770, d_r_acc=46, d_f_acc=53\n",
      "Step19500, d_r_loss=0.727, d_f_loss=0.653 g_loss=0.807, d_r_acc=34, d_f_acc=62\n",
      "Step19800, d_r_loss=0.669, d_f_loss=0.689 g_loss=0.789, d_r_acc=49, d_f_acc=53\n",
      "Step20100, d_r_loss=0.654, d_f_loss=0.651 g_loss=0.799, d_r_acc=57, d_f_acc=67\n",
      "Step20400, d_r_loss=0.686, d_f_loss=0.665 g_loss=0.777, d_r_acc=51, d_f_acc=57\n",
      "Step20700, d_r_loss=0.703, d_f_loss=0.655 g_loss=0.790, d_r_acc=45, d_f_acc=67\n",
      "Step21000, d_r_loss=0.680, d_f_loss=0.664 g_loss=0.793, d_r_acc=53, d_f_acc=61\n",
      "Step21300, d_r_loss=0.685, d_f_loss=0.686 g_loss=0.799, d_r_acc=51, d_f_acc=53\n",
      "Step21600, d_r_loss=0.665, d_f_loss=0.665 g_loss=0.794, d_r_acc=57, d_f_acc=64\n",
      "Step21900, d_r_loss=0.690, d_f_loss=0.668 g_loss=0.807, d_r_acc=47, d_f_acc=61\n",
      "Step22200, d_r_loss=0.707, d_f_loss=0.665 g_loss=0.813, d_r_acc=48, d_f_acc=53\n",
      "Step22500, d_r_loss=0.713, d_f_loss=0.698 g_loss=0.785, d_r_acc=38, d_f_acc=52\n",
      "Step22800, d_r_loss=0.658, d_f_loss=0.652 g_loss=0.769, d_r_acc=58, d_f_acc=64\n",
      "Step23100, d_r_loss=0.712, d_f_loss=0.658 g_loss=0.775, d_r_acc=44, d_f_acc=67\n",
      "Step23400, d_r_loss=0.669, d_f_loss=0.655 g_loss=0.775, d_r_acc=55, d_f_acc=62\n",
      "Step23700, d_r_loss=0.689, d_f_loss=0.684 g_loss=0.772, d_r_acc=50, d_f_acc=61\n",
      "Step24000, d_r_loss=0.667, d_f_loss=0.722 g_loss=0.778, d_r_acc=56, d_f_acc=53\n",
      "Step24300, d_r_loss=0.656, d_f_loss=0.691 g_loss=0.743, d_r_acc=62, d_f_acc=60\n",
      "Step24600, d_r_loss=0.667, d_f_loss=0.663 g_loss=0.794, d_r_acc=61, d_f_acc=60\n",
      "Step24900, d_r_loss=0.677, d_f_loss=0.699 g_loss=0.810, d_r_acc=50, d_f_acc=56\n",
      "Step25200, d_r_loss=0.676, d_f_loss=0.702 g_loss=0.783, d_r_acc=54, d_f_acc=52\n",
      "Step25500, d_r_loss=0.684, d_f_loss=0.670 g_loss=0.798, d_r_acc=52, d_f_acc=65\n",
      "Step25800, d_r_loss=0.686, d_f_loss=0.672 g_loss=0.806, d_r_acc=47, d_f_acc=55\n",
      "Step26100, d_r_loss=0.693, d_f_loss=0.680 g_loss=0.780, d_r_acc=42, d_f_acc=50\n",
      "Step26400, d_r_loss=0.718, d_f_loss=0.678 g_loss=0.788, d_r_acc=49, d_f_acc=49\n",
      "Step26700, d_r_loss=0.685, d_f_loss=0.675 g_loss=0.782, d_r_acc=48, d_f_acc=62\n",
      "Step27000, d_r_loss=0.683, d_f_loss=0.670 g_loss=0.804, d_r_acc=53, d_f_acc=61\n",
      "Step27300, d_r_loss=0.694, d_f_loss=0.655 g_loss=0.827, d_r_acc=50, d_f_acc=60\n",
      "Step27600, d_r_loss=0.666, d_f_loss=0.664 g_loss=0.768, d_r_acc=58, d_f_acc=64\n",
      "Step27900, d_r_loss=0.642, d_f_loss=0.693 g_loss=0.768, d_r_acc=58, d_f_acc=55\n",
      "Step28200, d_r_loss=0.656, d_f_loss=0.703 g_loss=0.797, d_r_acc=60, d_f_acc=47\n",
      "Step28500, d_r_loss=0.706, d_f_loss=0.660 g_loss=0.773, d_r_acc=39, d_f_acc=57\n",
      "Step28800, d_r_loss=0.654, d_f_loss=0.668 g_loss=0.777, d_r_acc=56, d_f_acc=59\n",
      "Step29100, d_r_loss=0.651, d_f_loss=0.675 g_loss=0.777, d_r_acc=62, d_f_acc=61\n",
      "Step29400, d_r_loss=0.677, d_f_loss=0.683 g_loss=0.807, d_r_acc=57, d_f_acc=55\n",
      "Step29700, d_r_loss=0.641, d_f_loss=0.711 g_loss=0.778, d_r_acc=56, d_f_acc=46\n",
      "Step30000, d_r_loss=0.656, d_f_loss=0.701 g_loss=0.783, d_r_acc=58, d_f_acc=59\n",
      "Step30300, d_r_loss=0.682, d_f_loss=0.663 g_loss=0.808, d_r_acc=55, d_f_acc=60\n",
      "Step30600, d_r_loss=0.686, d_f_loss=0.701 g_loss=0.780, d_r_acc=59, d_f_acc=51\n",
      "Step30900, d_r_loss=0.704, d_f_loss=0.687 g_loss=0.767, d_r_acc=42, d_f_acc=57\n",
      "Step31200, d_r_loss=0.702, d_f_loss=0.650 g_loss=0.763, d_r_acc=48, d_f_acc=66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step31500, d_r_loss=0.653, d_f_loss=0.664 g_loss=0.783, d_r_acc=67, d_f_acc=63\n",
      "Step31800, d_r_loss=0.680, d_f_loss=0.713 g_loss=0.790, d_r_acc=51, d_f_acc=50\n",
      "Step32100, d_r_loss=0.718, d_f_loss=0.677 g_loss=0.752, d_r_acc=46, d_f_acc=62\n",
      "Step32400, d_r_loss=0.715, d_f_loss=0.676 g_loss=0.786, d_r_acc=38, d_f_acc=57\n",
      "Step32700, d_r_loss=0.693, d_f_loss=0.678 g_loss=0.771, d_r_acc=54, d_f_acc=59\n",
      "Step33000, d_r_loss=0.657, d_f_loss=0.694 g_loss=0.759, d_r_acc=57, d_f_acc=52\n",
      "Step33300, d_r_loss=0.692, d_f_loss=0.669 g_loss=0.757, d_r_acc=44, d_f_acc=64\n",
      "Step33600, d_r_loss=0.672, d_f_loss=0.686 g_loss=0.763, d_r_acc=53, d_f_acc=58\n",
      "Step33900, d_r_loss=0.656, d_f_loss=0.620 g_loss=0.796, d_r_acc=61, d_f_acc=75\n",
      "Step34200, d_r_loss=0.658, d_f_loss=0.652 g_loss=0.777, d_r_acc=60, d_f_acc=60\n",
      "Step34500, d_r_loss=0.628, d_f_loss=0.686 g_loss=0.774, d_r_acc=67, d_f_acc=59\n",
      "Step34800, d_r_loss=0.650, d_f_loss=0.698 g_loss=0.768, d_r_acc=63, d_f_acc=50\n",
      "Step35100, d_r_loss=0.724, d_f_loss=0.686 g_loss=0.779, d_r_acc=42, d_f_acc=57\n",
      "Step35400, d_r_loss=0.659, d_f_loss=0.654 g_loss=0.793, d_r_acc=63, d_f_acc=61\n",
      "Step35700, d_r_loss=0.735, d_f_loss=0.676 g_loss=0.808, d_r_acc=41, d_f_acc=58\n",
      "Step36000, d_r_loss=0.675, d_f_loss=0.685 g_loss=0.778, d_r_acc=47, d_f_acc=57\n",
      "Step36300, d_r_loss=0.717, d_f_loss=0.655 g_loss=0.780, d_r_acc=51, d_f_acc=65\n",
      "Step36600, d_r_loss=0.686, d_f_loss=0.684 g_loss=0.759, d_r_acc=57, d_f_acc=53\n",
      "Step36900, d_r_loss=0.728, d_f_loss=0.662 g_loss=0.770, d_r_acc=46, d_f_acc=56\n",
      "Step37200, d_r_loss=0.648, d_f_loss=0.752 g_loss=0.757, d_r_acc=57, d_f_acc=50\n",
      "Step37500, d_r_loss=0.695, d_f_loss=0.730 g_loss=0.774, d_r_acc=40, d_f_acc=49\n",
      "Step37800, d_r_loss=0.711, d_f_loss=0.681 g_loss=0.773, d_r_acc=43, d_f_acc=53\n",
      "Step38100, d_r_loss=0.670, d_f_loss=0.654 g_loss=0.776, d_r_acc=54, d_f_acc=64\n",
      "Step38400, d_r_loss=0.692, d_f_loss=0.706 g_loss=0.766, d_r_acc=42, d_f_acc=54\n",
      "Step38700, d_r_loss=0.690, d_f_loss=0.698 g_loss=0.785, d_r_acc=51, d_f_acc=50\n",
      "Step39000, d_r_loss=0.642, d_f_loss=0.696 g_loss=0.742, d_r_acc=60, d_f_acc=53\n",
      "Step39300, d_r_loss=0.684, d_f_loss=0.680 g_loss=0.772, d_r_acc=47, d_f_acc=63\n",
      "Step39600, d_r_loss=0.707, d_f_loss=0.675 g_loss=0.759, d_r_acc=51, d_f_acc=58\n",
      "Step39900, d_r_loss=0.719, d_f_loss=0.663 g_loss=0.778, d_r_acc=45, d_f_acc=61\n",
      "Step40200, d_r_loss=0.649, d_f_loss=0.675 g_loss=0.784, d_r_acc=54, d_f_acc=64\n",
      "Step40500, d_r_loss=0.680, d_f_loss=0.681 g_loss=0.788, d_r_acc=54, d_f_acc=56\n",
      "Step40800, d_r_loss=0.682, d_f_loss=0.672 g_loss=0.774, d_r_acc=53, d_f_acc=57\n",
      "Step41100, d_r_loss=0.670, d_f_loss=0.725 g_loss=0.804, d_r_acc=50, d_f_acc=48\n",
      "Step41400, d_r_loss=0.689, d_f_loss=0.709 g_loss=0.767, d_r_acc=49, d_f_acc=51\n",
      "Step41700, d_r_loss=0.694, d_f_loss=0.684 g_loss=0.775, d_r_acc=52, d_f_acc=56\n",
      "Step42000, d_r_loss=0.715, d_f_loss=0.651 g_loss=0.763, d_r_acc=47, d_f_acc=64\n",
      "Step42300, d_r_loss=0.680, d_f_loss=0.633 g_loss=0.787, d_r_acc=47, d_f_acc=66\n",
      "Step42600, d_r_loss=0.643, d_f_loss=0.680 g_loss=0.770, d_r_acc=58, d_f_acc=63\n",
      "Step42900, d_r_loss=0.678, d_f_loss=0.663 g_loss=0.804, d_r_acc=57, d_f_acc=58\n",
      "Step43200, d_r_loss=0.684, d_f_loss=0.678 g_loss=0.776, d_r_acc=59, d_f_acc=60\n",
      "Step43500, d_r_loss=0.666, d_f_loss=0.678 g_loss=0.770, d_r_acc=54, d_f_acc=57\n",
      "Step43800, d_r_loss=0.705, d_f_loss=0.677 g_loss=0.785, d_r_acc=42, d_f_acc=58\n",
      "Step44100, d_r_loss=0.689, d_f_loss=0.702 g_loss=0.755, d_r_acc=46, d_f_acc=56\n",
      "Step44400, d_r_loss=0.644, d_f_loss=0.677 g_loss=0.776, d_r_acc=57, d_f_acc=57\n",
      "Step44700, d_r_loss=0.690, d_f_loss=0.706 g_loss=0.770, d_r_acc=52, d_f_acc=54\n",
      "Step45000, d_r_loss=0.697, d_f_loss=0.669 g_loss=0.767, d_r_acc=52, d_f_acc=52\n",
      "Step45300, d_r_loss=0.698, d_f_loss=0.704 g_loss=0.776, d_r_acc=52, d_f_acc=52\n",
      "Step45600, d_r_loss=0.674, d_f_loss=0.704 g_loss=0.762, d_r_acc=50, d_f_acc=50\n",
      "Step45900, d_r_loss=0.691, d_f_loss=0.699 g_loss=0.793, d_r_acc=51, d_f_acc=54\n",
      "Step46200, d_r_loss=0.688, d_f_loss=0.685 g_loss=0.760, d_r_acc=50, d_f_acc=53\n",
      "Step46500, d_r_loss=0.686, d_f_loss=0.693 g_loss=0.775, d_r_acc=51, d_f_acc=56\n",
      "Step46800, d_r_loss=0.677, d_f_loss=0.694 g_loss=0.771, d_r_acc=48, d_f_acc=58\n"
     ]
    }
   ],
   "source": [
    "train_gan()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ff78ccf",
   "metadata": {},
   "source": [
    "## With the above metrics, we can see that the generator gets increasingly better at \"tricking\" the discriminator into misclassifying images due to the fake/generated images become more and more realistic. Probably an excessive number of epochs as the accuracy and loss metrics seem to level out about half way through the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc059a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan_new.gif'\n",
    "imgs = []\n",
    "dir_name = \"C:/Users/Michael/results_new9/\"\n",
    "f = 0\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = sorted( filter( os.path.isfile,glob.glob(dir_name + 'generated_plot_*') ), key=os.path.getmtime )\n",
    "    for file in filenames:\n",
    "        f +=1\n",
    "        image = imageio.imread(file)\n",
    "        imgs.append(image)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(file)\n",
    "    writer.append_data(image)\n",
    "    imgs.append(image)\n",
    "imageio.mimsave(\"results_new9/gifGAN_new1.gif\",imgs, fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed53831",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.embed_file(anim_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
